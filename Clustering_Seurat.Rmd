---
title: "R Notebook"
output: html_notebook
---

## load the package 

```{r}
suppressPackageStartupMessages({
  library(scater)
  library(SC3)
  library(Seurat)
  library(ggplot2)
  library(SingleCellExperiment)
  library(DuoClustering2018)
  library(ggplot2)
  library(mclust)
})
```

ref: https://satijalab.org/seurat/v3.2/pbmc3k_tutorial.html

Seurat use a different object called seurat object.


```{r}
seurat_obj <- CreateSeuratObject(counts = counts(sce_zheng))
```
#Standard pre-processing workflow

##QC and selecting cells for further analysis


```{r}
# The [[ operator can add columns to object metadata. This is a great place to stash QC stats
#seurat_obj[["percent.mt"]] <- PercentageFeatureSet(seurat_obj, pattern = "^MT-")
```

Visualize QC metrics as a violin plot

```{r}
#VlnPlot(seurat_obj, features = c("nFeature_RNA", "nCount_RNA", "percent.mt"), ncol = 3)
VlnPlot(seurat_obj, features = c("nFeature_RNA", "nCount_RNA" ), ncol = 2)
```

FeatureScatter is typically used to visualize feature-feature relationships, but can be used for anything calculated by the object, i.e. columns in object metadata, PC scores etc.

```{r}

# plot1 <- FeatureScatter(seurat_obj, feature1 = "nCount_RNA", feature2 = "percent.mt")
plot2 <- FeatureScatter(seurat_obj, feature1 = "nCount_RNA", feature2 = "nFeature_RNA")
plot2
```

In the example below, we visualize QC metrics, and use these to filter cells.

We filter cells that have unique feature counts over 2,500 or less than 200

We filter cells that have >5% mitochondrial counts (This part is not completed because we don't have MT rna label in the data)

```{r}
seurat_obj_filter <- subset(seurat_obj, subset = nFeature_RNA > 200 & nFeature_RNA < 2500)

```

### Normalization:

By default, we employ a global-scaling normalization method "LogNormalize" that normalizes the feature expression measurements for each cell by the total expression, multiplies this by a scale factor (10,000 by default), and log-transforms the result. 

```{r}
seurat_obj_lognorm <- NormalizeData(seurat_obj_filter)

```

### Identification of highly variable features (feature selection)
We next calculate a subset of features that exhibit high cell-to-cell variation in the dataset (i.e, they are highly expressed in some cells, and lowly expressed in others). 

By default, we return 2,000 features per dataset. These will be used in downstream analysis, like PCA.


```{r}
# Use all default paraameters
seurat_obj_featuresel <- FindVariableFeatures(seurat_obj_lognorm)

```


Identify the 10 most highly variable genes

plot variable features with and without labels
```{r}
top10 <- head(VariableFeatures(seurat_obj_featuresel), 10)

plot1 <- VariableFeaturePlot(seurat_obj_featuresel)
plot2 <- LabelPoints(plot = plot1, points = top10, repel = TRUE)
plot1 + plot2
```

## Scaling the data

Next, we apply a linear transformation ('scaling') that is a standard pre-processing step prior to dimensional reduction techniques like PCA. The ScaleData function:

1. Shifts the expression of each gene, so that the mean expression across cells is 0

2. Scales the expression of each gene, so that the variance across cells is 1

This step gives equal weight in downstream analyses, so that highly-expressed genes do not dominate

The results of this are stored in pbmc[["RNA"]]@scale.data

```{r}
all.genes <- rownames(seurat_obj_featuresel)
seurat_obj_scale <- ScaleData(seurat_obj_featuresel, features = all.genes)
```


## Perform linear dimensional reduction

Next we perform PCA on the scaled data. By default, only the previously determined variable features are used as input, but can be defined using features argument if you wish to choose a different subset.

```{r}
seurat_obj_pca <- RunPCA(seurat_obj_scale, features = VariableFeatures(object = seurat_obj_scale))
```


```{r}
print(seurat_obj_pca[["pca"]], dims = 1:5, nfeatures = 4)

```

```{r}
VizDimLoadings(seurat_obj_pca, dims = 1:2, reduction = "pca")

```


```{r}
DimPlot(seurat_obj_pca, reduction = "pca")

```

In particular DimHeatmap allows for easy exploration of the primary sources of heterogeneity in a dataset, and can be useful when trying to decide which PCs to include for further downstream analyses. Both cells and features are ordered according to their PCA scores. Setting cells to a number plots the 'extreme' cells on both ends of the spectrum, which dramatically speeds plotting for large datasets. Though clearly a supervised analysis, we find this to be a valuable tool for exploring correlated feature sets.

```{r}
DimHeatmap(seurat_obj_pca, dims = 1:3, cells = 500, balanced = TRUE)

```

## Determine the 'dimensionality' of the dataset

To overcome the extensive technical noise in any single feature for scRNA-seq data, Seurat clusters cells based on their PCA scores, with each PC essentially representing a 'metafeature' that combines information across a correlated feature set. The top principal components therefore represent a robust compression of the dataset. 

In Macosko et al, we implemented a resampling test inspired by the JackStraw procedure. We randomly permute a subset of the data (1% by default) and rerun PCA, constructing a 'null distribution' of feature scores, and repeat this procedure. We identify 'significant' PCs as those who have a strong enrichment of low p-value features.

```{r}
# NOTE: This process can take a long time for big datasets
seurat_obj_jackstraw <- JackStraw(seurat_obj_pca, num.replicate = 100)
seurat_obj_score <- ScoreJackStraw(seurat_obj_jackstraw, dims = 1:20)
```

The JackStrawPlot function provides a visualization tool for comparing the distribution of p-values for each PC with a uniform distribution (dashed line). 'Significant' PCs will show a strong enrichment of features with low p-values (solid curve above the dashed line). 

```{r}
JackStrawPlot(seurat_obj_score, dims = 1:20)

```

An alternative heuristic method generates an 'Elbow plot': a ranking of principle components based on the percentage of variance explained by each one (ElbowPlot function). 

In this example, we can observe an 'elbow' around PC10-11, suggesting that the majority of true signal is captured in the first 11 PCs.
```{r}
ElbowPlot(seurat_obj_score)
```



Identifying the true dimensionality of a dataset -- can be challenging/uncertain for the user. We therefore suggest these three approaches to consider. 

The first is more supervised, exploring PCs to determine relevant sources of heterogeneity, and could be used in conjunction with GSEA for example. 

The second implements a statistical test based on a random null model, but is time-consuming for large datasets, and may not return a clear PC cutoff. 

The third is a heuristic that is commonly used, and can be calculated instantly. 

## Cluster the cells

Importantly, the distance metric which drives the clustering analysis (based on previously identified PCs) remains the same. However, our approach to partioning the cellular distance matrix into clusters has dramatically improved. Our approach was heavily inspired by recent manuscripts which applied graph-based clustering approaches to scRNA-seq data [SNN-Cliq, Xu and Su, Bioinformatics, 2015] and CyTOF data [PhenoGraph, Levine et al., Cell, 2015].

Briefly, these methods embed cells in a graph structure - for example a K-nearest neighbor (KNN) graph, with edges drawn between cells with similar feature expression patterns, and then attempt to partition this graph into highly interconnected 'quasi-cliques' or 'communities'.

As in PhenoGraph, we first construct a KNN graph based on the euclidean distance in PCA space, and refine the edge weights between any two cells based on the shared overlap in their local neighborhoods (Jaccard similarity). This step is performed using the FindNeighbors function, and takes as input the previously defined dimensionality of the dataset (first 10 PCs).

To cluster the cells, we next apply modularity optimization techniques such as the Louvain algorithm (default) or SLM [SLM, Blondel et al., Journal of Statistical Mechanics], to iteratively group cells together, with the goal of optimizing the standard modularity function. The FindClusters function implements this procedure, and contains a resolution parameter that sets the 'granularity' of the downstream clustering, with increased values leading to a greater number of clusters. We find that setting this parameter between 0.4-1.2 typically returns good results for single-cell datasets of around 3K cells. Optimal resolution often increases for larger datasets. The clusters can be found using the Idents function.

```{r}
seurat_obj_neighb <- FindNeighbors(seurat_obj_score)
```


```{r}
seurat_obj_cluster <- FindClusters(seurat_obj_neighb)
```

Look at cluster IDs of the first 5 cells

```{r}
head(Idents(seurat_obj_cluster), 20)
```

```{r}
DimPlot(seurat_obj_cluster, label = TRUE, pt.size = 0.5) + NoLegend()
```

calculate ARI from package mclust.

```{r}
seurat_out <- data.frame(Idents(seurat_obj_cluster))
seurat_out$labels = gsub("^\\d+|\\d+$", "", row.names(seurat_out))
adjustedRandIndex(seurat_out$Idents.seurat_obj_cluster., seurat_out$labels)
```



